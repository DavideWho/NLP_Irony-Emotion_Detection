{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10460899,"sourceType":"datasetVersion","datasetId":6476265}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#from google.colab import drive\n#drive.mount('/content/drive', force_remount=True)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3311,"status":"ok","timestamp":1736780758943,"user":{"displayName":"Davide Sgroi","userId":"11889222768285529626"},"user_tz":-60},"id":"-RxSVWZfPBf_","outputId":"f11ed24e-4d1b-434f-a0d1-980ad4a03124","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n#!pip install --upgrade nltk\n!pip install emoji","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2643,"status":"ok","timestamp":1736780761583,"user":{"displayName":"Davide Sgroi","userId":"11889222768285529626"},"user_tz":-60},"id":"mEkgirnEPQ9M","outputId":"a3b00389-5a4b-403e-b8f4-ec18fc3fcf50","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize, pos_tag\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer, AdamW, get_scheduler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom torch import nn\nfrom torch.optim import Adam\nfrom tqdm import tqdm\nimport emoji\n\nnltk.download('punkt')\nnltk.download('punkt_tab')\nnltk.download('averaged_perceptron_tagger_eng')\nnltk.download('stopwords')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11144,"status":"ok","timestamp":1736779714347,"user":{"displayName":"Davide Sgroi","userId":"11889222768285529626"},"user_tz":-60},"id":"Q6PQzsJAN63t","outputId":"91dacd1e-2945-4136-f9ed-3c834a4a6a8c","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:40:07.825221Z","iopub.execute_input":"2025-01-24T13:40:07.825535Z","iopub.status.idle":"2025-01-24T13:40:10.952583Z","shell.execute_reply.started":"2025-01-24T13:40:07.825511Z","shell.execute_reply":"2025-01-24T13:40:10.951875Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"#root = '/content/drive/MyDrive/NLP/Davide/irony_davide_datasets/'\nroot = '/kaggle/input/irony-datasets/'\n\nx_train = pd.read_csv(root + 'x_train.csv')\nx_test = pd.read_csv(root + 'x_test.csv')\nx_val = pd.read_csv(root + 'x_val.csv')\ny_train = pd.read_csv(root + 'y_train.csv')\ny_test = pd.read_csv(root + 'y_test.csv')\ny_val = pd.read_csv(root + 'y_val.csv')","metadata":{"id":"-fr0KV2zPbtv","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:40:10.953530Z","iopub.execute_input":"2025-01-24T13:40:10.953915Z","iopub.status.idle":"2025-01-24T13:40:10.983141Z","shell.execute_reply.started":"2025-01-24T13:40:10.953893Z","shell.execute_reply":"2025-01-24T13:40:10.982445Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"print(len(x_train))\nprint(len(x_test))\nprint(len(x_val))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1736780763642,"user":{"displayName":"Davide Sgroi","userId":"11889222768285529626"},"user_tz":-60},"id":"JSRKwd6vsXvK","outputId":"718263ab-57ba-4319-b541-7c4db9790507","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:40:10.984798Z","iopub.execute_input":"2025-01-24T13:40:10.984992Z","iopub.status.idle":"2025-01-24T13:40:10.989490Z","shell.execute_reply.started":"2025-01-24T13:40:10.984974Z","shell.execute_reply":"2025-01-24T13:40:10.988732Z"}},"outputs":[{"name":"stdout","text":"2862\n764\n191\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"hyperparameters = {\n    \"epochs\": 10,\n    \"learning_rate\": 3e-5,   #1e-5\n    \"batch_size\": 32,  #accura...\n    \"dropout\": 0.1,   # o 0.3\n    \"weight_decay\": 1e-4,  #1e-3\n    \"stopwords\": False,\n    \"language_model\": \"microsoft/deberta-v3-base\", #\"vinai/bertweet-base\", #bert-base-uncased\n    \"layers\": 1,\n    \"h_dim\": 768,\n    \"bilstm\": True,\n    \"patience\": 5,\n    \"min_delta\": 0.01,\n    \"extra_features\": 5,\n    \"threshold\": 0.5\n}","metadata":{"id":"8xBXFIfzaJeU","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:40:10.990435Z","iopub.execute_input":"2025-01-24T13:40:10.990689Z","iopub.status.idle":"2025-01-24T13:40:11.002891Z","shell.execute_reply.started":"2025-01-24T13:40:10.990663Z","shell.execute_reply":"2025-01-24T13:40:11.002271Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n\n    def __init__(self, x, hashtag_count, avg_ironic_ratio, ironic_hashtag_count, non_ironic_hashtag_count, hashtag_irony_index, y, stopwords):  #nn_count\n        # x e y sono series di pandas\n        tokens_litt = [nltk.word_tokenize(text, language='english') for text in list(x)]\n        text_clean = []\n\n        if stopwords:\n            for sentence in tqdm(tokens_litt, desc='Tokenizing ... '):\n                text_clean.append(' '.join([w.lower() for w in sentence if\n                                            not w.lower() in nltk.corpus.stopwords.words(\"english\")]))\n        else:\n            for sentence in tqdm(tokens_litt, desc='Tokenizing ... '):\n                text_clean.append(' '.join([w.lower() for w in sentence]))\n            # ogni token Ã¨ separato dall'altro con uno spazio\n\n        self.texts = text_clean\n        self.labels = [torch.tensor(label) for label in y]\n        \n        self.hashtag_count = [torch.tensor(count) for count in hashtag_count]\n        self.avg_ironic_ratio = [torch.tensor(ratio) for ratio in avg_ironic_ratio]\n        self.ironic_hashtag_count = [torch.tensor(count) for count in ironic_hashtag_count]\n        self.non_ironic_hashtag_count = [torch.tensor(count) for count in non_ironic_hashtag_count]\n        self.hashtag_irony_index = [torch.tensor(index) for index in hashtag_irony_index]\n\n    def classes(self):\n        return self.labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        batch_texts = self.texts[idx]\n        batch_labels = np.array(self.labels[idx])\n        \n        batch_hashtag_count = np.array(self.hashtag_count[idx])\n        batch_avg_ironic_ratio = np.array(self.avg_ironic_ratio[idx])\n        batch_ironic_hashtag_count = np.array(self.ironic_hashtag_count[idx])\n        batch_non_ironic_hashtag_count = np.array(self.non_ironic_hashtag_count[idx])\n        batch_hashtag_irony_index = np.array(self.hashtag_irony_index[idx])\n\n        return batch_texts, batch_hashtag_count, batch_avg_ironic_ratio, batch_ironic_hashtag_count, batch_non_ironic_hashtag_count, batch_hashtag_irony_index, batch_labels","metadata":{"id":"zVZdes9oODcg","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:40:11.003602Z","iopub.execute_input":"2025-01-24T13:40:11.003780Z","iopub.status.idle":"2025-01-24T13:40:11.015732Z","shell.execute_reply.started":"2025-01-24T13:40:11.003763Z","shell.execute_reply":"2025-01-24T13:40:11.014885Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"#x_test","metadata":{"id":"33wUwS-Cdosc","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:40:11.016602Z","iopub.execute_input":"2025-01-24T13:40:11.016841Z","iopub.status.idle":"2025-01-24T13:40:11.031529Z","shell.execute_reply.started":"2025-01-24T13:40:11.016822Z","shell.execute_reply":"2025-01-24T13:40:11.030767Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"","metadata":{"id":"2qiMe9wkPdkz"}},{"cell_type":"code","source":"train_dataset = Dataset(x_train['text'], x_train['hashtag_count'], x_train['avg_ironic_ratio'], x_train['ironic_hashtag_count'], x_train['non_ironic_hashtag_count'], x_train['hashtag_irony_index'], y_train.squeeze(1), hyperparameters[\"stopwords\"])\nval_dataset = Dataset(x_val['text'], x_val['hashtag_count'], x_val['avg_ironic_ratio'], x_val['ironic_hashtag_count'], x_val['non_ironic_hashtag_count'], x_val['hashtag_irony_index'], y_val.squeeze(1), hyperparameters[\"stopwords\"])\ntest_dataset = Dataset(x_test['text'], x_test['hashtag_count'], x_test['avg_ironic_ratio'], x_test['ironic_hashtag_count'], x_test['non_ironic_hashtag_count'], x_test['hashtag_irony_index'], y_test.squeeze(1), hyperparameters[\"stopwords\"])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1603,"status":"ok","timestamp":1736780826740,"user":{"displayName":"Davide Sgroi","userId":"11889222768285529626"},"user_tz":-60},"id":"ElhQQnd4Z27l","outputId":"1fe153bf-1f6e-4386-9ecc-f88e38d688dc","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:40:11.032449Z","iopub.execute_input":"2025-01-24T13:40:11.032761Z","iopub.status.idle":"2025-01-24T13:40:11.945803Z","shell.execute_reply.started":"2025-01-24T13:40:11.032729Z","shell.execute_reply":"2025-01-24T13:40:11.945206Z"}},"outputs":[{"name":"stderr","text":"Tokenizing ... : 100%|ââââââââââ| 2862/2862 [00:00<00:00, 240240.52it/s]\nTokenizing ... : 100%|ââââââââââ| 191/191 [00:00<00:00, 311595.51it/s]\nTokenizing ... : 100%|ââââââââââ| 764/764 [00:00<00:00, 371616.40it/s]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=5, min_delta=0.0):\n\n        self.patience = patience\n        self.min_delta = min_delta              # valore minimo di decrescita della loss di validazione all'epoca corrente\n                                                # per asserire che c'Ã¨ un miglioramenti della loss\n        self.counter = 0                        # contatore delle epoche di pazienza\n        self.early_stop = False                 # flag di early stop\n        self.min_validation_loss = torch.inf    # valore corrente ottimo della loss di validazione\n\n    def __call__(self, validation_loss):\n        # chiamata in forma funzionale dell'oggetto di classe EarlySopping\n\n        if (validation_loss + self.min_delta) >= self.min_validation_loss:  # la loss di validazione non decresce\n            self.counter += 1                                               # incrementiamo il contatore delle epoche di pazienza\n            if self.counter >= self.patience:\n                self.early_stop = True\n                print(\"Early stop!\")\n        else:                                               # c'Ã¨ un miglioramento della loss:\n            self.min_validation_loss = validation_loss      # consideriamo la loss corrente\n                                                            # come nuova loss ottimale\n            self.counter = 0                                # e azzeriamo il contatore di pazienza\n","metadata":{"id":"NyAtahwyZ2Il","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:40:11.948164Z","iopub.execute_input":"2025-01-24T13:40:11.948373Z","iopub.status.idle":"2025-01-24T13:40:11.953454Z","shell.execute_reply.started":"2025-01-24T13:40:11.948355Z","shell.execute_reply":"2025-01-24T13:40:11.952644Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class ClassifierDeep(nn.Module):\n\n    def __init__(self, hdim, dropout, model_name, extra_features = hyperparameters[\"extra_features\"]): #hyperparameters[\"extra_features\"]\n        super(ClassifierDeep, self).__init__()\n        config = AutoConfig.from_pretrained(model_name)\n        self.lm_model = AutoModel.from_pretrained(model_name, config=config)\n        self.classifier = nn.Sequential(\n            nn.Linear(hdim + extra_features, 1),\n            #nn.Dropout(dropout),\n            #nn.ReLU(),\n            #nn.Linear(16, 1),\n            nn.Sigmoid()\n            )\n\n\n    def forward(self, input_id_text, attention_mask, hashtag_count, avg_ironic_ratio, ironic_hashtag_count, non_ironic_hashtag_count, hashtag_irony_index):\n        output = self.lm_model(input_id_text, attention_mask).last_hidden_state\n        output = output[:,0,:]\n        #output = torch.cat((output, emoji_count.unsqueeze(1), has_ironic_emoji.unsqueeze(1), has_non_ironic_emoji.unsqueeze(1), nn_count.unsqueeze(1)), dim=1)\n        output = torch.cat((output, hashtag_count.unsqueeze(1), avg_ironic_ratio.unsqueeze(1), ironic_hashtag_count.unsqueeze(1), non_ironic_hashtag_count.unsqueeze(1), hashtag_irony_index.unsqueeze(1)), dim=1)\n        return self.classifier(output)","metadata":{"id":"PUHJ5LoDoPev","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:40:36.741402Z","iopub.execute_input":"2025-01-24T13:40:36.741706Z","iopub.status.idle":"2025-01-24T13:40:36.747663Z","shell.execute_reply.started":"2025-01-24T13:40:36.741683Z","shell.execute_reply":"2025-01-24T13:40:36.746807Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def train_loop(model, dataloader, tokenizer, loss, optimizer, device, threshold):\n    model.train()\n\n    epoch_acc = 0\n    epoch_loss = 0\n\n    for batch_texts, batch_hashtag_count, batch_avg_ironic_ratio, batch_ironic_hashtag_count, batch_non_ironic_hashtag_count, batch_hashtag_irony_index, batch_labels in dataloader:\n\n        optimizer.zero_grad()\n\n        tokens = tokenizer(list(batch_texts),\n                           add_special_tokens=True,\n                           return_tensors='pt',\n                           padding='max_length',\n                           max_length = 128,\n                           truncation=True)\n\n        input_id_texts = tokens['input_ids'].squeeze(1).to(device)\n        mask_texts = tokens['attention_mask'].squeeze(1).to(device)\n    \n        batch_hashtag_count = batch_hashtag_count.float().to(device)\n        batch_avg_ironic_ratio = batch_avg_ironic_ratio.float().to(device)\n        batch_ironic_hashtag_count = batch_ironic_hashtag_count.float().to(device)\n        batch_non_ironic_hashtag_count = batch_non_ironic_hashtag_count.float().to(device)\n        batch_hashtag_irony_index = batch_hashtag_irony_index.float().to(device)\n        batch_labels = batch_labels.float().to(device)\n\n\n        output = model(input_id_texts, mask_texts, batch_hashtag_count, batch_avg_ironic_ratio, batch_ironic_hashtag_count, batch_non_ironic_hashtag_count, batch_hashtag_irony_index).squeeze(1)\n\n        # la loss Ã¨ una CrossEntropyLoss, al suo interno ha\n        # la logsoftmax + negative log likelihood loss\n        batch_loss = loss(output, batch_labels)\n        batch_loss.backward()\n        optimizer.step()\n\n        epoch_loss += batch_loss.item()\n\n        # per calcolare l'accuracy devo generare le predizioni\n        # applicando manualmente la logsoftmax\n        preds = (output > 0.6).float()  # Soglia di 0.5 per la classificazione binaria\n        epoch_acc += (preds == batch_labels).sum().item()\n\n        batch_labels = batch_labels.detach().cpu()\n        input_id_texts = input_id_texts.detach().cpu()\n        mask_texts = mask_texts.detach().cpu()\n        \n        batch_hashtag_count = batch_hashtag_count.detach().cpu()\n        batch_avg_ironic_ratio = batch_avg_ironic_ratio.detach().cpu()\n        batch_ironic_hashtag_count = batch_ironic_hashtag_count.detach().cpu()\n        batch_non_ironic_hashtag_count = batch_non_ironic_hashtag_count.detach().cpu()\n        batch_hashtag_irony_index = batch_hashtag_irony_index.detach().cpu()\n        output = output.detach().cpu()\n\n\n    return epoch_loss/len(dataloader), epoch_acc\n","metadata":{"id":"rU8RtKwO8tN0","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:40:11.972387Z","iopub.execute_input":"2025-01-24T13:40:11.972665Z","iopub.status.idle":"2025-01-24T13:40:11.987595Z","shell.execute_reply.started":"2025-01-24T13:40:11.972644Z","shell.execute_reply":"2025-01-24T13:40:11.986980Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\ndef test_loop(model, dataloader, tokenizer, loss, device, threshold):\n    model.eval()\n\n    epoch_acc = 0\n    epoch_loss = 0\n\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch_texts, batch_hashtag_count, batch_avg_ironic_ratio, batch_ironic_hashtag_count, batch_non_ironic_hashtag_count, batch_hashtag_irony_index, batch_labels in dataloader:\n\n            tokens = tokenizer(list(batch_texts),\n                               add_special_tokens=True,\n                               return_tensors='pt',\n                               padding='max_length',\n                               max_length=128,\n                               truncation=True)\n            input_id_texts = tokens['input_ids'].squeeze(1).to(device)\n            mask_texts = tokens['attention_mask'].squeeze(1).to(device)\n            batch_hashtag_count = batch_hashtag_count.float().to(device)\n            batch_avg_ironic_ratio = batch_avg_ironic_ratio.float().to(device)\n            batch_ironic_hashtag = batch_ironic_hashtag_count.float().to(device)\n            batch_non_ironic_hashtag = batch_non_ironic_hashtag_count.float().to(device)\n            batch_hashtag_irony_index = batch_hashtag_irony_index.float().to(device)\n            batch_labels = batch_labels.float().to(device)\n\n            output = model(input_id_texts, mask_texts, batch_hashtag_count, batch_avg_ironic_ratio, batch_ironic_hashtag, batch_non_ironic_hashtag, batch_hashtag_irony_index).squeeze(1)\n\n            # Calcola la loss\n            batch_loss = loss(output, batch_labels)\n            epoch_loss += batch_loss.item()\n\n            # Genera predizioni binarie\n            preds = (output > 0.6).float()  #dovrei mettere threshold\n\n            # Calcola l'accuracy\n            epoch_acc += (preds == batch_labels).sum().item()\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(batch_labels.cpu().numpy())\n\n            batch_labels = batch_labels.detach().cpu()\n            input_id_texts = input_id_texts.detach().cpu()\n            mask_texts = mask_texts.detach().cpu()\n            batch_hashtag_count = batch_hashtag_count.detach().cpu()\n            batch_avg_ironic_ratio = batch_avg_ironic_ratio.detach().cpu()\n            batch_ironic_hashtag = batch_ironic_hashtag_count.detach().cpu()\n            batch_non_ironic_hashtag = batch_non_ironic_hashtag_count.detach().cpu()\n            batch_hashtag_irony_index = batch_hashtag_irony_index.detach().cpu()\n            output = output.detach().cpu()\n\n    f1 = f1_score(all_labels, all_preds)\n\n    return epoch_loss/len(dataloader), epoch_acc, f1\n","metadata":{"id":"r1yFWCCu-Pah","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:40:11.988471Z","iopub.execute_input":"2025-01-24T13:40:11.988751Z","iopub.status.idle":"2025-01-24T13:40:12.005794Z","shell.execute_reply.started":"2025-01-24T13:40:11.988724Z","shell.execute_reply":"2025-01-24T13:40:12.005173Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"'''def unfreeze_layers(model, freeze_up_to_layer):\n    # Itera su tutti i parametri del modello\n    for name, param in model.named_parameters():\n        # Gestisce l'encoder e altri componenti\n        if 'encoder.layer' in name:\n            # Estrai il numero del layer\n            try:\n                layer_number = int(name.split('.')[2])  # esempio \"encoder.layer.11.attention.self.query.weight\"\n            except ValueError:\n                continue  # salta se non riesci a ottenere il numero del layer\n\n            # Congela i parametri fino al livello specificato\n            if layer_number < freeze_up_to_layer:\n                param.requires_grad = False\n            else:\n                param.requires_grad = True\n\n        # Gestione degli embeddings (se vuoi congelarli o meno)\n        elif 'embeddings' in name:\n            param.requires_grad = False  # Congela gli embeddings (o cambia se vuoi sbloccarli)\n\n        # Gestione della parte del pooler e della testa di classificazione (se devi allenare queste parti)\n        elif 'pooler' in name or 'classifier' in name:\n            param.requires_grad = True  # Assicurati che questi componenti siano allenati\n\n        # Stampa dello stato di \"requires_grad\" per ogni parametro\n        print(f\"{name} - requires_grad = {param.requires_grad}\")\n'''","metadata":{"id":"fO1WVAgVWssY","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:40:12.006541Z","iopub.execute_input":"2025-01-24T13:40:12.006811Z","iopub.status.idle":"2025-01-24T13:40:12.025802Z","shell.execute_reply.started":"2025-01-24T13:40:12.006782Z","shell.execute_reply":"2025-01-24T13:40:12.025213Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'def unfreeze_layers(model, freeze_up_to_layer):\\n    # Itera su tutti i parametri del modello\\n    for name, param in model.named_parameters():\\n        # Gestisce l\\'encoder e altri componenti\\n        if \\'encoder.layer\\' in name:\\n            # Estrai il numero del layer\\n            try:\\n                layer_number = int(name.split(\\'.\\')[2])  # esempio \"encoder.layer.11.attention.self.query.weight\"\\n            except ValueError:\\n                continue  # salta se non riesci a ottenere il numero del layer\\n\\n            # Congela i parametri fino al livello specificato\\n            if layer_number < freeze_up_to_layer:\\n                param.requires_grad = False\\n            else:\\n                param.requires_grad = True\\n\\n        # Gestione degli embeddings (se vuoi congelarli o meno)\\n        elif \\'embeddings\\' in name:\\n            param.requires_grad = False  # Congela gli embeddings (o cambia se vuoi sbloccarli)\\n\\n        # Gestione della parte del pooler e della testa di classificazione (se devi allenare queste parti)\\n        elif \\'pooler\\' in name or \\'classifier\\' in name:\\n            param.requires_grad = True  # Assicurati che questi componenti siano allenati\\n\\n        # Stampa dello stato di \"requires_grad\" per ogni parametro\\n        print(f\"{name} - requires_grad = {param.requires_grad}\")\\n'"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"def train_test(model, epochs, optimizer, device, threshold, train_data, test_data,\n               batch_size, model_name, train_loss_fn,\n               test_loss_fn=None,         # non necessariamente train e test loss devono differire\n               early_stopping=None,       # posso addstrare senza early stopping\n               val_data=None,             # e in questo caso non c'Ã¨ validation set\n               scheduler=None,            # possibile scheduler per monitorare l'andamento di un iperparametro\n              ):    \n\n    # Congelamento progressivo all'inizio\n    #unfreeze_layers(model, freeze_up_to_layer)\n\n    train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n    val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=batch_size)\n    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n\n    # check sulle funzioni di loss\n    #if test_loss_fn == None:\n    #    test_loss_fn = train_loss_fn\n\n    # liste dei valori di loss e accuracy epoca per epoca per il plot\n    train_loss = []\n    validation_loss = []\n    test_loss = []\n    train_acc = []\n    validation_acc = []\n    test_acc = []\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    all_emojis = list(emoji.EMOJI_DATA.keys())\n    emoji_tokens = [emoji.demojize(e) for e in all_emojis]\n    tokenizer.add_tokens(emoji_tokens)\n    tokenizer.add_tokens([\"@user\"])\n    model.lm_model.resize_token_embeddings(len(tokenizer))\n\n    # Ciclo di addestramento con early stopping\n    for epoch in range(1,epochs+1):\n\n        epoch_train_loss, epoch_train_acc = train_loop(model, train_dataloader, tokenizer, train_loss_fn, optimizer, device, threshold)\n        train_loss.append(epoch_train_loss)\n        train_acc.append(epoch_train_acc/len(train_data))\n\n       # Validation se Ã¨ presente la callback di early stopping\n        if early_stopping != None:\n            epoch_validate_loss, epoch_validate_acc, _, = test_loop(model, val_dataloader, tokenizer, test_loss_fn, device, threshold)\n            validation_loss.append(epoch_validate_loss)\n            validation_acc.append(epoch_validate_acc/len(val_data))\n\n        # Test\n        epoch_test_loss, epoch_test_acc, epcoch_f1 = test_loop(model, test_dataloader, tokenizer, test_loss_fn, device, threshold)\n        test_loss.append(epoch_test_loss)\n        test_acc.append(epoch_test_acc/len(test_data))\n\n\n        val_loss_str = f'Validation loss: {epoch_validate_loss:6.4f} 'if early_stopping != None else ' '\n        val_acc_str = f'Validation accuracy: {(epoch_validate_acc/len(val_data)):6.4f} ' if early_stopping != None else ' '\n        print(f\"\\nTrain loss: {epoch_train_loss:6.4f} {val_loss_str} Test loss: {epoch_test_loss:6.4f}\")\n        print(f\"Train accuracy: {(epoch_train_acc/len(train_data)):6.4f} {val_acc_str}Test accuracy: {(epoch_test_acc/len(test_data)):6.4f}\")\n\n\n        # Early stopping\n        if early_stopping != None:\n            early_stopping(epoch_validate_loss)\n            if early_stopping.early_stop:\n                break\n\n        # Sblocca i layer ogni 'freeze_every_n_epochs'\n        #if epoch % freeze_every_n_epochs == 0:\n        #    freeze_up_to_layer = max(0, freeze_up_to_layer - 1)  # Sblocca un layer\n        #    unfreeze_layers(model, freeze_up_to_layer)\n\n    return train_loss, validation_loss, test_loss, train_acc, validation_acc, test_acc, epcoch_f1","metadata":{"id":"Z7OPTRm6TqBk","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:40:12.026610Z","iopub.execute_input":"2025-01-24T13:40:12.026851Z","iopub.status.idle":"2025-01-24T13:40:12.042678Z","shell.execute_reply.started":"2025-01-24T13:40:12.026831Z","shell.execute_reply":"2025-01-24T13:40:12.041830Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using {device} device\")\n\nmodel = ClassifierDeep(\n                    hyperparameters[\"h_dim\"],\n                    hyperparameters[\"dropout\"],\n                    hyperparameters[\"language_model\"]).to(device)\nprint(model)\n\n# Calcoliamo il numero totale dei parametri del modello\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Numbero totale dei parametri: {total_params}\")\n\n#threshold = hyperparameters['threshold']\ncriterion = nn.BCELoss()\noptimizer = AdamW(model.parameters(), lr=hyperparameters['learning_rate'], weight_decay=hyperparameters['weight_decay'])\n\n###### Linear Warmup + Decay ######\n# Calcolo dei passi totali\n#total_steps = len(train_dataset) // hyperparameters['batch_size'] * hyperparameters['epochs']\n\n# Passi di warmup (ad esempio, 10% del totale)\n#warmup_steps = int(0.1 * total_steps)\n\n# Creazione del scheduler\n#scheduler = get_scheduler(\n#    name=\"linear\",  # Tipo di scheduler   ---> PROVARE COSINE ---> provare con OTTIMIZZATORE SGD INVECE CHE ADAM\n#    optimizer=optimizer,  # Ottimizzatore che stai usando\n#    num_warmup_steps=warmup_steps,\n#    num_training_steps=total_steps\n#)\n###################################\n\n\n# Creiamo la callback di early stopping da passare al nostro metodo di addestramento\nearly_stopping = EarlyStopping(patience=hyperparameters['patience'], min_delta=hyperparameters['min_delta'])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1090,"status":"ok","timestamp":1736780888003,"user":{"displayName":"Davide Sgroi","userId":"11889222768285529626"},"user_tz":-60},"id":"h7s_f8sGdHF6","outputId":"f8f5e067-d9ad-4507-e2bd-7628905f7285","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:40:12.043474Z","iopub.execute_input":"2025-01-24T13:40:12.043732Z","iopub.status.idle":"2025-01-24T13:40:14.070120Z","shell.execute_reply.started":"2025-01-24T13:40:12.043713Z","shell.execute_reply":"2025-01-24T13:40:14.069206Z"}},"outputs":[{"name":"stdout","text":"Using cuda device\nClassifierDeep(\n  (lm_model): DebertaV2Model(\n    (embeddings): DebertaV2Embeddings(\n      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n      (dropout): StableDropout()\n    )\n    (encoder): DebertaV2Encoder(\n      (layer): ModuleList(\n        (0-11): 12 x DebertaV2Layer(\n          (attention): DebertaV2Attention(\n            (self): DisentangledSelfAttention(\n              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n              (pos_dropout): StableDropout()\n              (dropout): StableDropout()\n            )\n            (output): DebertaV2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n              (dropout): StableDropout()\n            )\n          )\n          (intermediate): DebertaV2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaV2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n            (dropout): StableDropout()\n          )\n        )\n      )\n      (rel_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n    )\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=773, out_features=1, bias=True)\n    (1): Dropout(p=0.1, inplace=False)\n    (2): ReLU()\n    (3): Linear(in_features=16, out_features=1, bias=True)\n    (4): Sigmoid()\n  )\n)\nNumbero totale dei parametri: 183832343\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"#!export CUDA_LAUNCH_BLOCKING=1\n#torch.cuda.empty_cache()","metadata":{"id":"k21QobgGdK_B","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:40:14.070993Z","iopub.execute_input":"2025-01-24T13:40:14.071510Z","iopub.status.idle":"2025-01-24T13:40:14.074876Z","shell.execute_reply.started":"2025-01-24T13:40:14.071478Z","shell.execute_reply":"2025-01-24T13:40:14.074089Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Congela i layer fino al numero specificato\n#freeze_up_to_layer = 4\n#unfreeze_layers(model, freeze_up_to_layer)\n\n# Recupera i parametri che richiedono il calcolo del gradiente (i parametri non congelati)\n#params_to_train = [param for param in model.parameters() if param.requires_grad]\n\n# Verifica se ci sono parametri da allenare\n#if len(params_to_train) == 0:\n#  raise ValueError(\"Non ci sono parametri da allenare. Verifica che il congelamento e sblocco dei layer siano corretti.\")\n\n# Creazione dell'ottimizzatore con i parametri non congelati\ncriterion = nn.BCELoss()\n#optimizer = AdamW(params_to_train, lr=hyperparameters['learning_rate'])\noptimizer = AdamW(model.parameters(), lr=hyperparameters['learning_rate'], weight_decay = hyperparameters['weight_decay'])\n\n# Ora puoi chiamare la routine di addestramento\ntrain_loss, validation_loss, test_loss, train_acc, validation_acc, test_acc, f1 = train_test(\n    model,\n    hyperparameters['epochs'],\n    optimizer,\n    device,\n    hyperparameters['threshold'],\n    train_dataset,\n    test_dataset,  # Assicurati che questo sia il dataset corretto\n    hyperparameters['batch_size'],\n    hyperparameters['language_model'],\n    criterion,\n    criterion,\n    early_stopping=early_stopping,\n    val_data=val_dataset  # Qui passa il dataset di validazione\n)\nprint(f'F1 score: {f1}')  ","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"U3YfvmxKSpc9","outputId":"c33752eb-fbcf-4fb6-bbe2-3695aba34e96","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:40:41.778413Z","iopub.execute_input":"2025-01-24T13:40:41.778703Z","iopub.status.idle":"2025-01-24T13:40:43.019306Z","shell.execute_reply.started":"2025-01-24T13:40:41.778681Z","shell.execute_reply":"2025-01-24T13:40:43.018095Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-0e22a138659d>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Ora puoi chiamare la routine di addestramento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m train_loss, validation_loss, test_loss, train_acc, validation_acc, test_acc, f1 = train_test(\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-aadbef8b7f6e>\u001b[0m in \u001b[0;36mtrain_test\u001b[0;34m(model, epochs, optimizer, device, threshold, train_data, test_data, batch_size, model_name, train_loss_fn, test_loss_fn, early_stopping, val_data, scheduler)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mepoch_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_train_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_train_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mtrain_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_train_acc\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-05aea191dd0a>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(model, dataloader, tokenizer, loss, optimizer, device, threshold)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_id_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_hashtag_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_avg_ironic_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ironic_hashtag_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_non_ironic_hashtag_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_hashtag_irony_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# la loss Ã¨ una CrossEntropyLoss, al suo interno ha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-92263d9b164a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_id_text, attention_mask, hashtag_count, avg_ironic_ratio, ironic_hashtag_count, non_ironic_hashtag_count, hashtag_irony_index)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#output = torch.cat((output, emoji_count.unsqueeze(1), has_ironic_emoji.unsqueeze(1), has_non_ironic_emoji.unsqueeze(1), nn_count.unsqueeze(1)), dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhashtag_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_ironic_ratio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mironic_hashtag_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_ironic_hashtag_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhashtag_irony_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x1 and 16x1)"],"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (32x1 and 16x1)","output_type":"error"}],"execution_count":21},{"cell_type":"code","source":"'''from sklearn.model_selection import ParameterGrid\nimport matplotlib.pyplot as plt\n\nparam_grid = {\n    \"learning_rate\": [1e-4,1e-5,1e-6],\n    \"batch_size\": [32,128],\n    \"dropout\": [0.1, 0.2],\n    \"weight_decay\": [1e-2,1e-4],\n    \"epochs\": [5],\n    \"threshold\": [0.4,0.6]\n}\n\nbest_val_acc_acc=0\nbest_val_loss_acc=1\nbest_val_f1_acc=0\nbest_params_acc = {}\nbest_val_acc_loss=0\nbest_val_loss_loss=1\nbest_val_f1_loss=0\nbest_params_loss = {}\nbest_val_acc_f1=0\nbest_val_loss_f1=1\nbest_val_f1_f1=0\nbest_params_f1 = {}\n\ngrid = ParameterGrid(param_grid)\nfor params in grid:\n  print(\"----------------------------------------------------------------\")\n  print(params)\n  # Acquisiamo il device su cui effettueremo il training\n  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n  print(f\"Using {device} device\")\n\n  model = ClassifierDeep(\n                      hyperparameters[\"h_dim\"],\n                      hyperparameters[\"dropout\"],\n                      hyperparameters[\"language_model\"]).to(device)\n  print(model)\n\n  # Calcoliamo il numero totale dei parametri del modello\n  total_params = sum(p.numel() for p in model.parameters())\n  print(f\"Numbero totale dei parametri: {total_params}\")\n\n  criterion = nn.BCELoss()\n  optimizer = AdamW(model.parameters(), lr=params[\"learning_rate\"], weight_decay=params[\"weight_decay\"])\n\n  # Creiamo la callback di early stopping da passare al nostro metodo di addestramento\n  early_stopping = EarlyStopping(patience=hyperparameters['patience'], min_delta=hyperparameters['min_delta'])\n\n\n\n\n  train_loss, val_loss, test_loss, train_acc, val_acc, test_acc, f1 = train_test(\n    model,\n    hyperparameters['epochs'],\n    optimizer,\n    device,\n    hyperparameters['threshold'],\n    train_dataset,\n    test_dataset,  # Assicurati che questo sia il dataset corretto\n    hyperparameters['batch_size'],\n    hyperparameters['language_model'],\n    criterion,\n    criterion,\n    early_stopping=early_stopping,\n    val_data=val_dataset  # Qui passa il dataset di validazione\n  )\n\n  print(f1)\n\n  if val_acc[-1] > best_val_acc_acc:\n    best_val_acc_acc = val_acc[-1]\n    best_val_loss_acc = val_loss[-1]\n    best_val_f1_acc = f1\n    best_params_acc = params\n\n  if val_loss[-1] < best_val_loss_loss:\n    best_val_acc_loss = val_acc[-1]\n    best_val_loss_loss = val_loss[-1]\n    best_val_f1_loss = f1\n    best_params_loss = params\n\n  if f1 > best_val_f1_f1:\n    best_val_acc_f1 = val_acc[-1]\n    best_val_loss_f1 = val_loss[-1]\n    best_val_f1_f1 = f1\n    best_params_f1 = params\n\n  fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n\n  axs[0].plot(train_loss, label='training loss')\n  axs[0].plot(val_loss, label='validation loss')\n  axs[0].legend(loc='upper right')\n  axs[0].set_ylim(0,1)\n\n  axs[1].plot(train_acc, label='training accuracy')\n  axs[1].plot(val_acc, label='validation accuracy')\n  axs[1].legend(loc='lower right')\n  axs[1].set_ylim(0,1)\n\n  plt.show()","metadata":{"colab":{"background_save":true},"id":"BMvXitWEexOz","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:40:15.931823Z","iopub.status.idle":"2025-01-24T13:40:15.932082Z","shell.execute_reply":"2025-01-24T13:40:15.931964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, axs = plt.subplots(1, 2, figsize=(20, 10))\n\naxs[0].plot(train_loss, label='training loss')\naxs[0].plot(validation_loss, label='validation loss')\naxs[0].plot(test_loss, label='test loss')\naxs[0].legend(loc='upper right')\naxs[0].set_ylim(0,1)\n\naxs[1].plot(train_acc, label='training accuracy')\naxs[1].plot(validation_acc, label='validation accuracy')\naxs[1].plot(test_acc, label='test accuracy')\naxs[1].legend(loc='lower right')\naxs[1].set_ylim(0,1)\n","metadata":{"colab":{"background_save":true},"id":"aS_AAnCbdQ3J","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:40:15.932804Z","iopub.status.idle":"2025-01-24T13:40:15.933100Z","shell.execute_reply":"2025-01-24T13:40:15.932961Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}