# -*- coding: utf-8 -*-
"""NLP_Davide_DatasetsCreation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R0ZIlV39OKZqtIQgWersUOl5YfPCrCpd
"""

from google.colab import drive
drive.mount('/content/drive')

!pip uninstall -y nltk
!pip install --upgrade nltk
!pip install emoji

import pandas as pd
import numpy as np
import sklearn
from sklearn.model_selection import train_test_split
import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize, pos_tag
from transformers import AutoConfig, AutoModel, AutoTokenizer, AdamW
import matplotlib.pyplot as plt
import seaborn as sns
import torch
from torch import nn
from torch.optim import Adam
from tqdm import tqdm
import emoji

nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('stopwords')

root = '/content/drive/MyDrive/NLP/Challenge_2024/irony/'
with open(root+"train_text.txt") as f_input:
    data = [line for line in f_input]

texts = pd.DataFrame(data, columns=['text'])
texts

labels = pd.read_csv(root+"train_labels.txt", header=None)
labels

data = pd.concat([texts, labels], axis=1)
data.columns = ['text', 'label']

data['emoji_count'] = data['text'].apply(lambda text: sum(char in emoji.EMOJI_DATA.keys() for char in text))

emojidf = (texts['text'].apply(lambda text: [token.chars for token in emoji.analyze(text, join_emoji=True)
                       if isinstance(token.value, emoji.EmojiMatch)]).explode().value_counts()
                      .rename_axis('Smiley').rename('Count').reset_index())
emoji_ironic = (data[data['label'] == 1]['text'].apply(lambda text: [token.chars for token in emoji.analyze(text, join_emoji=True)
                       if isinstance(token.value, emoji.EmojiMatch)]).explode().value_counts()
                      .rename_axis('Smiley').rename('Count').reset_index())

emojidf_comparison = emojidf.merge(emoji_ironic, on='Smiley', how='outer', suffixes=('total', 'ironic')).fillna(0)
emojidf_comparison['ironic_ratio'] = emojidf_comparison['Countironic'] / emojidf_comparison['Counttotal']
emojidf_comparison = emojidf_comparison.sort_values(by='ironic_ratio', ascending=False)

emojidf_comparison = emojidf_comparison[emojidf_comparison['Counttotal']>3]
emojidf_comparison

relevant_emojis = emojidf_comparison[emojidf_comparison['Counttotal']>3]

emoji_ironic_pool = relevant_emojis[relevant_emojis['ironic_ratio']>0.6]
print(emoji_ironic_pool)
emoji_ironic_pool = emoji_ironic_pool['Smiley'].tolist()
print(emoji_ironic_pool)
print("-----------")
emoji_non_ironic_pool = relevant_emojis[relevant_emojis['ironic_ratio']<0.4]
print(emoji_non_ironic_pool)
emoji_non_ironic_pool = emoji_non_ironic_pool['Smiley'].tolist()
print(emoji_non_ironic_pool)

data["has_ironic_emoji"] = data["text"].apply(lambda x: 1 if any(char in emoji_ironic_pool for char in x) else 0)
data["has_non_ironic_emoji"] = data["text"].apply(lambda x: 1 if any(char in emoji_non_ironic_pool for char in x) else 0)

data

data['text'] = data['text'].apply(lambda text: emoji.demojize(text))

# Verifica il tokenizzatore
text = "This is a test sentence."
tokens = word_tokenize(text)
print("Tokenized:", tokens)

# Verifica il POS tagging
tags = pos_tag(tokens)
print("POS Tags:", tags)

# Funzione per contare i PoS
def extract_pos_counts(text):
    tokens = word_tokenize(text)  # Tokenizza il testo
    tagged = pos_tag(tokens)  # Tagga le parole con NLTK
    pos_counts = {
        'JJ': 0,   # Aggettivi
        'VB': 0,   # Verbi base
        'NN': 0,   # Sostantivi singolari
        'RB': 0    # Avverbi
    }
    for _, tag in tagged:
        if tag.startswith('JJ'):  # Aggettivi
            pos_counts['JJ'] += 1
        elif tag.startswith('VB'):  # Verbi
            pos_counts['VB'] += 1
        elif tag.startswith('NN'):  # Sostantivi
            pos_counts['NN'] += 1
        elif tag.startswith('RB'):  # Avverbi
            pos_counts['RB'] += 1
    return pos_counts

# Estrai i conteggi dei PoS e espandi in colonne
data['pos_counts'] = data['text'].apply(extract_pos_counts)
df_pos = data['pos_counts'].apply(pd.Series)  # Espandi il dizionario
data = pd.concat([data, df_pos], axis=1)  # Unisci al DataFrame originale

# Aggiungi la colonna delle etichette
data['label'] = labels

# Verifica la presenza delle colonne
print(data.columns)  # Assicurati che 'JJ', 'VB', 'NN', 'RB' e 'label' siano presenti

# Calcola la matrice di correlazione
correlation_matrix = data[['JJ', 'VB', 'NN', 'RB', 'label']].corr()
print(correlation_matrix)

data

# Espandi i risultati in colonne separate
df_pos = data['pos_counts'].apply(pd.Series)

# Evita i duplicati: Rimuovi le colonne esistenti prima di concatenare
df = data.drop(columns=['JJ', 'VB', 'NN', 'RB'], errors='ignore')  # Rimuove se esistono già
df = pd.concat([data, df_pos], axis=1)


# Calcolare la matrice di correlazione
correlation_matrix = data[['JJ', 'VB', 'NN', 'RB', 'label']].corr()

# Dizionario per spiegare le abbreviazioni
pos_labels = {
    'JJ': 'Adjective (Aggettivi)',
    'VB': 'Verb (Verbi)',
    'NN': 'Noun (Sostantivi)',
    'RB': 'Adverb (Avverbi)',
    'label': 'Target (Ironia)'
}

# Rinominare colonne e righe nella matrice di correlazione
correlation_matrix.rename(columns=pos_labels, index=pos_labels, inplace=True)

# Visualizzare la matrice di correlazione
plt.figure(figsize=(10, 6))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Matrice di Correlazione tra PoS e Label")
plt.show()

# Calcolare la somma dei PoS per ciascuna classe (ironico e non ironico)
pos_summary = df.groupby('label')[['JJ', 'VB', 'NN', 'RB']].sum()

# Rinominare le etichette per chiarezza
pos_summary.index = ['Non Ironico', 'Ironico']

# Reset dell'indice e rinomina per il metodo melt
pos_summary = pos_summary.reset_index().rename(columns={'index': 'Etichetta'})

# Trasformare il DataFrame in formato "long" con melt
pos_summary_melted = pos_summary.melt(id_vars='Etichetta', var_name='PoS', value_name='Conteggio')

# Dizionario per la leggibilità delle etichette
pos_labels_graph = {
    'JJ': 'Aggettivi',
    'VB': 'Verbi',
    'NN': 'Sostantivi',
    'RB': 'Avverbi'
}
pos_summary_melted['PoS'] = pos_summary_melted['PoS'].map(pos_labels_graph)

# Creazione del grafico
plt.figure(figsize=(10, 6))
sns.barplot(x='PoS', y='Conteggio', hue='Etichetta', data=pos_summary_melted, palette='coolwarm')

# Aggiungere titolo e etichette
plt.title("Distribuzione dei PoS nei Tweet Ironici e Non Ironici")
plt.xlabel("Parti del discorso (PoS)")
plt.ylabel("Conteggio Totale")
plt.legend(title="Etichetta", loc='upper right')
plt.show()

# Aggiungi la colonna dei conteggi di sostantivi (NN) direttamente al DataFrame
data['NN_count'] = df_pos['NN']  # Se 'NN' non esiste, questa linea genera NaN

# Sostituisci eventuali NaN con 0 (facoltativo, per sicurezza)
data['NN_count'] = data['NN_count'].fillna(0).astype(int)


# Rimuovere le colonne specificate
data = data.drop(columns=['JJ', 'VB', 'NN', 'RB'], errors='ignore')

# Rimuovere la riga 'pos_counts' se presente
data = data[data.index != 'pos_counts']

data

data = data.drop(columns=['pos_counts'])
data

from sklearn.model_selection import train_test_split

x_train, x_temp, y_train, y_temp = train_test_split(data.drop(columns=['label']), data['label'], test_size=0.4, random_state=42)

x_test, x_val, y_test, y_val = train_test_split(x_temp, y_temp, test_size=0.8, random_state=42)

import pandas as pd

# Salva i dataframe in file CSV
x_train.to_csv('x_train.csv', index=False)
x_test.to_csv('x_test.csv', index=False)
x_val.to_csv('x_val.csv', index=False)
y_train.to_csv('y_train.csv', index=False)
y_test.to_csv('y_test.csv', index=False)
y_val.to_csv('y_val.csv', index=False)

from google.colab import files

# Scarica i file
files.download('x_train.csv')
files.download('x_test.csv')
files.download('x_val.csv')
files.download('y_train.csv')
files.download('y_test.csv')
files.download('y_val.csv')